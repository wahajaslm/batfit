{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BatFit LoRA Training on Colab\n",
        "Use this notebook to fine-tune BatFit adapters on a Colab GPU while mirroring the CLI workflow.\n",
        "\n",
        "**Workflow overview**\n",
        "1. Clone or verify the repo checkout.\n",
        "2. Install the Colab-friendly requirements.\n",
        "3. (Optional) mount Google Drive for persistent artifacts.\n",
        "4. Configure hyperparameters via `BATFIT_*` env vars.\n",
        "5. Step through each modular stage (prompt, data, tokenizer, trainer) to debug.\n",
        "6. Launch training via `trainer.train()`, `main()`, or the CLI fallback.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 \u2013 Clone or verify repository\n",
        "This repo is public, so point `GIT_REPO`/`BATFIT_REPO_DIR` where you like. If a `.git` folder already exists, the cell simply reports the current working directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "repo_url = os.environ.get('GIT_REPO', 'https://github.com/wahajaslm/batfit.git')\n",
        "repo_dir = Path(os.environ.get('BATFIT_REPO_DIR', 'batfit'))\n",
        "if Path('.git').exists():\n",
        "    print(f'Already inside repo: {Path.cwd()}')\n",
        "else:\n",
        "    if not repo_dir.exists():\n",
        "        subprocess.check_call(['git', 'clone', repo_url, str(repo_dir)])\n",
        "    os.chdir(repo_dir)\n",
        "    print(f'Working directory -> {Path.cwd()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 \u2013 Install Colab dependencies\n",
        "Installs the lightweight requirements (transformers, datasets, peft, etc.). Run once per runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q -r requirements-colab.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 \u2013 (Optional) Mount Drive\n",
        "Only needed when you want checkpoints/logs to persist beyond the Colab session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drive"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 \u2013 Configure training knobs\n",
        "Set `BATFIT_BASE_MODEL`, `BATFIT_MAX_LEN`, `BATFIT_EPOCHS`, or any other `BATFIT_*` overrides (batch size, learning rate, etc.) before building datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['BATFIT_BASE_MODEL'] = os.environ.get('BATFIT_BASE_MODEL', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0')\n",
        "os.environ['BATFIT_MAX_LEN'] = os.environ.get('BATFIT_MAX_LEN', '768')\n",
        "os.environ['BATFIT_EPOCHS'] = os.environ.get('BATFIT_EPOCHS', '2')\n",
        "print('Base model:', os.environ['BATFIT_BASE_MODEL'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 \u2013 Modular training pipeline\n",
        "Each sub-step below calls a helper from `scripts/train_lora.py` so you can inspect intermediate artifacts without launching a full training run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5a. Load system prompt\n",
        "`resolve_system_prompt()` gives precedence to `BATFIT_SYSTEM_PROMPT`, then `data/common/prompts/system.txt`, then the script default.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.append(str(repo_root))\n",
        "\n",
        "from scripts.train_lora import (\n",
        "    load_from_manifest,\n",
        "    resolve_system_prompt,\n",
        "    prepare_tokenizer,\n",
        "    tokenize_splits,\n",
        "    resolve_device,\n",
        "    prepare_model,\n",
        "    build_trainer,\n",
        "    main,\n",
        ")\n",
        "\n",
        "system_prompt = resolve_system_prompt()\n",
        "print('System prompt loaded (chars):', len(system_prompt))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5b. Load manifest-defined datasets\n",
        "`load_from_manifest()` applies manifest weights, normalizes JSONL rows, and optionally carves a validation split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_raw, val_raw = load_from_manifest()\n",
        "print('Train rows:', len(train_raw))\n",
        "print('Val rows:', len(val_raw) if val_raw is not None else 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5c. Prepare tokenizer\n",
        "`prepare_tokenizer()` ensures the chat template has a pad token and right-side padding before batching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = prepare_tokenizer()\n",
        "print('Tokenizer vocab size:', tokenizer.vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5d. Tokenize datasets\n",
        "`tokenize_splits()` converts normalized rows into LM inputs/labels so you can inspect lengths and spot data issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset, val_dataset = tokenize_splits(train_raw, val_raw, tokenizer, system_prompt)\n",
        "print('Tokenized train len:', len(train_dataset))\n",
        "print('Tokenized val len:', len(val_dataset) if val_dataset is not None else 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5e. Resolve device + dtype\n",
        "`resolve_device()` checks CUDA/MPS availability and determines the correct `device_map`/dtype combo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_cuda, use_mps, device_map, dtype = resolve_device()\n",
        "print('CUDA:', use_cuda, 'MPS:', use_mps, 'device_map:', device_map, 'dtype:', dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5f. Build base model + LoRA adapters\n",
        "`prepare_model()` loads the base checkpoint, disables cache, enables gradient checkpointing, and injects LoRA modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = prepare_model(os.environ.get('BATFIT_BASE_MODEL', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'), dtype, device_map, use_mps)\n",
        "print('Model with LoRA adapters ready.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5g. Assemble Trainer\n",
        "`build_trainer()` wires the model, tokenizer, datasets, and default hyperparameters together. Rerun if you tweak settings above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = build_trainer(model, tokenizer, train_dataset, val_dataset)\n",
        "print('Trainer ready.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6a \u2013 Train interactively\n",
        "Uncomment `trainer.train()` once you're satisfied with the inspected artifacts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6b \u2013 Run `main()` end-to-end\n",
        "Call `main()` if you want the exact CLI behavior without stepping through each helper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from scripts.train_lora import main\n",
        "# _ = main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6c \u2013 Legacy CLI fallback\n",
        "Shelling out remains available for parity with older instructions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python scripts/train_lora.py\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "train_batfit.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}